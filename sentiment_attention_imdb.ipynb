{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_attention_imdb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarsar/sentiment_with_attention/blob/master/sentiment_attention_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "W6g57dioQjWG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentiment Classification with Hierarchical Attention\n",
        "In this notebook I train a sentiment classification model with a GRU model. I also add an attention layer so as to visualize what the model is learnining and considers important for sentiment classification. Code adapted from [here](https://github.com/ilivans/tf-rnn-attention) (thanks to the author for releasing his thesis code). Attention is inspired by this [paper](http://www.aclweb.org/anthology/N16-1174)."
      ]
    },
    {
      "metadata": {
        "id": "hI46RXvdQjWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb8794e5-a3a4-4a96-8e0b-d890cab34eb7"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib.rnn import GRUCell\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn as rnn\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from keras.datasets import imdb\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "b2dcyzUoQjWQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69q11mNmQjWT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ]
    },
    {
      "metadata": {
        "id": "QIxBTTeTQjWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 10000\n",
        "INDEX_FROM = 3\n",
        "SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_SIZE = 150\n",
        "ATTENTION_SIZE = 50\n",
        "KEEP_PROB = 0.8\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
        "DELTA = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFfDRcOsQjWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "metadata": {
        "id": "6Oq8zyGqQjWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6733adaf-8a9c-4c07-a502-6ce309f46aa4"
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SUGUwDGWQjWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "d505fe97-7bed-4fe9-b5b0-7ac8d492ed32"
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "KTawoNz3QjWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "2d4a295f-94ab-4153-fd74-c47c924b0759"
      },
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "       list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 2, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "       list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 8738, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 2, 267, 162, 711, 37, 5900, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 5664, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 2, 68, 1830, 19, 6571, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 2, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "       ...,\n",
              "       list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "       list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "       list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 2, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "xk0xzkBCQjWi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sequence Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "7q6bXBHwQjWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Returns the largest wordid as the vocab_size\n",
        "def get_vocabulary_size(X):\n",
        "    return max([max(x) for x in X]) + 1  # plus the 0th word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCEgsWDJQjWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Removes words not in vocabulary\n",
        "def fit_in_vocabulary(X, voc_size):\n",
        "    return [[w for w in x if w < voc_size] for x in X]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CkjgpWebQjWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Zero padding in sequences\n",
        "def zero_pad(X, seq_len):\n",
        "    return np.array([x[:seq_len - 1] + [0] * max(seq_len - len(x), 1) for x in X])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E03Am-lEQjWp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = get_vocabulary_size(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7nTbX43nQjWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63ca5a78-c1e8-43ee-8826-47f8af3210b9"
      },
      "cell_type": "code",
      "source": [
        "print(vocabulary_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ly9sMFlkQjWv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test = fit_in_vocabulary(X_test, vocabulary_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYg-OS53QjWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf00d23f-7f77-4ca0-ca77-c496ca8609f2"
      },
      "cell_type": "code",
      "source": [
        "print(get_vocabulary_size(X_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UOJgEcgWQjW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b7569cf-bd4e-4d99-ad6f-d8fbcc4d0944"
      },
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "QTTEe1zzQjW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = zero_pad(X_train, SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fsOI4vgQjW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7cae1a3d-0013-4a1e-b72e-da05f3272f11"
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,   14,   22, ...,    0,    0,    0],\n",
              "       [   1,  194, 1153, ...,    0,    0,    0],\n",
              "       [   1,   14,   47, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   1,   11,    6, ...,    0,    0,    0],\n",
              "       [   1, 1446, 7079, ...,    0,    0,    0],\n",
              "       [   1,   17,    6, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "IwpLehe1QjXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qF9vb5EnQjXD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Placeholders for Model"
      ]
    },
    {
      "metadata": {
        "id": "EKZ1g8rjQjXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH])\n",
        "target_ph = tf.placeholder(tf.float32, [None])\n",
        "seq_len_ph = tf.placeholder(tf.int32, [None])\n",
        "keep_prob_ph = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5pMKsLDQjXF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer"
      ]
    },
    {
      "metadata": {
        "id": "m2xpCE6EQjXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
        "batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cgrjslOQjXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f003b84f-00dd-41f7-ed88-0d75c3b265eb"
      },
      "cell_type": "code",
      "source": [
        "batch_embedded"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 250, 100) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "WeA73uvZQjXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BI-RNN Layers"
      ]
    },
    {
      "metadata": {
        "id": "AgyPmek6QjXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gd28JpgQjXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "metadata": {
        "id": "3SkPcSu0QjXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
        "    if isinstance(inputs, tuple):\n",
        "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        # (T,B,D) => (B,T,D)\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
        "\n",
        "    # Trainable parameters\n",
        "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "    #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "    v = tf.tanh(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n",
        "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "    vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu)              # (B,T) shape also\n",
        "\n",
        "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dbqtdRLgQjXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### attention layer\n",
        "attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5jEGnCvQjXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Add DROPOUT (avoids overfitting)\n",
        "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTRGwWytQjXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fully Connected Layer"
      ]
    },
    {
      "metadata": {
        "id": "Gs2PEmQpQjXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
        "b = tf.Variable(tf.constant(0., shape=[1]))\n",
        "y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
        "y_hat = tf.squeeze(y_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtipsjVYQjXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cross-entropy loss and optimizer initialization"
      ]
    },
    {
      "metadata": {
        "id": "kBUPowm3QjXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss and optimizer initialization\n",
        "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MMhYEuuQjXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Accuracy Metric"
      ]
    },
    {
      "metadata": {
        "id": "FDj4o7J3QjXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzvGYmu2QjXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "fd36-zfBQjXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "F3L65F6zQjXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Determine batches\n",
        "def batch_generator(X, y, batch_size):\n",
        "    \"\"\"Primitive batch generator \n",
        "    \"\"\"\n",
        "    size = X.shape[0]\n",
        "    X_copy = X.copy()\n",
        "    y_copy = y.copy()\n",
        "    indices = np.arange(size)\n",
        "    np.random.shuffle(indices)\n",
        "    X_copy = X_copy[indices]\n",
        "    y_copy = y_copy[indices]\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size <= size:\n",
        "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
        "            i += batch_size\n",
        "        else:\n",
        "            i = 0\n",
        "            indices = np.arange(size)\n",
        "            np.random.shuffle(indices)\n",
        "            X_copy = X_copy[indices]\n",
        "            y_copy = y_copy[indices]\n",
        "            continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXRfUQTOQjXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Actual lengths of sequences\n",
        "seq_len_test = np.array([list(x).index(0) + 1 for x in X_test])\n",
        "seq_len_train = np.array([list(x).index(0) + 1 for x in X_train])\n",
        "\n",
        "# Batch generators\n",
        "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
        "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zILqSpDQjXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LUxwFRkjQjXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ac3f6dd4-21c5-450a-e1d1-40e6bd01b426"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    print(\"Start learning...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss_train = 0\n",
        "        loss_test = 0\n",
        "        accuracy_train = 0\n",
        "        accuracy_test = 0\n",
        "\n",
        "        print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
        "\n",
        "        # Training\n",
        "        num_batches = X_train.shape[0] // BATCH_SIZE\n",
        "        for b in range(num_batches):\n",
        "            x_batch, y_batch = next(train_batch_generator)\n",
        "            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
        "            loss_tr, acc, _ = sess.run([loss, accuracy, optimizer],\n",
        "                                       feed_dict={batch_ph: x_batch,\n",
        "                                                  target_ph: y_batch,\n",
        "                                                  seq_len_ph: seq_len,\n",
        "                                                  keep_prob_ph: KEEP_PROB})\n",
        "            accuracy_train += acc\n",
        "            loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
        "        accuracy_train /= num_batches\n",
        "\n",
        "        # Testing\n",
        "        num_batches = X_test.shape[0] // BATCH_SIZE\n",
        "        for b in range(num_batches):\n",
        "            x_batch, y_batch = next(test_batch_generator)\n",
        "            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
        "            loss_test_batch, acc = sess.run([loss, accuracy],\n",
        "                                            feed_dict={batch_ph: x_batch,\n",
        "                                                       target_ph: y_batch,\n",
        "                                                       seq_len_ph: seq_len,\n",
        "                                                       keep_prob_ph: 1.0})\n",
        "            accuracy_test += acc\n",
        "            loss_test += loss_test_batch\n",
        "        accuracy_test /= num_batches\n",
        "        loss_test /= num_batches\n",
        "\n",
        "        print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
        "            loss_train, loss_test, accuracy_train, accuracy_test\n",
        "        ))\n",
        "    saver.save(sess, \"model/test-rnn\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start learning...\n",
            "epoch: 0\tloss: 0.440, val_loss: 0.429, acc: 0.719, val_acc: 0.801\n",
            "epoch: 1\tloss: 0.305, val_loss: 0.368, acc: 0.843, val_acc: 0.842\n",
            "epoch: 2\tloss: 0.271, val_loss: 0.332, acc: 0.886, val_acc: 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wWaC9HADQjXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62ea060a-b45d-4bfe-9de4-029c57429c08"
      },
      "cell_type": "code",
      "source": [
        "X_test.shape[0] // BATCH_SIZE"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "gtf_yp1uQjXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fdfcb43-2fd7-4fc6-8ba7-5157884e3a2e"
      },
      "cell_type": "code",
      "source": [
        "num_batches * 256"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "gmnDVmaqQjXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize Attention output"
      ]
    },
    {
      "metadata": {
        "id": "EG7twLlsQjXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.import_meta_graph(\"model/test-rnn.meta\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9uvgDi-QjXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a202ac1d-c134-4fa8-b140-934a8dd98a8a"
      },
      "cell_type": "code",
      "source": [
        "### calculate alpha coefficients for the first test example\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"model/test-rnn\")\n",
        "    x_batch_test, y_batch_test = X_test[:1], y_test[:1] # or sum((x_batch_test != 0)[0]) + 1\n",
        "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
        "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
        "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/test-rnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zN_cdIIrQjX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alphas_values = alphas_test[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41PEgulXQjX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70753033-c9da-47f1-b0ef-661180ffdf0d"
      },
      "cell_type": "code",
      "source": [
        "y_batch_test"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "f3u59IRJQjX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mapping from word to index"
      ]
    },
    {
      "metadata": {
        "id": "FGb5lAYaQjX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c55ad0e8-0525-4c45-c7cc-2968361976f7"
      },
      "cell_type": "code",
      "source": [
        "# Build correct mapping from word to index and inverse\n",
        "original_word_index = imdb.get_word_index()\n",
        "word_index = {word:index + INDEX_FROM for word, index in original_word_index.items()}\n",
        "word_index[\":PAD:\"] = 0\n",
        "word_index[\":START:\"] = 1\n",
        "word_index[\":UNK:\"] = 2\n",
        "index_word = {value:key for key,value in word_index.items()}\n",
        "# Represent the sample by words rather than indices\n",
        "words = list(map(index_word.get, x_batch_test[0]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_IVYrxbKQjX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "d512ed80-07a7-4ba2-b1df-c52465e13da8"
      },
      "cell_type": "code",
      "source": [
        "x_batch_test[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,  591,  202,   14,   31,    6,  717,   10,   10,    2,    2,\n",
              "          5,    4,  360,    7,    4,  177, 5760,  394,  354,    4,  123,\n",
              "          9, 1035, 1035, 1035,   10,   10,   13,   92,  124,   89,  488,\n",
              "       7944,  100,   28, 1668,   14,   31,   23,   27, 7479,   29,  220,\n",
              "        468,    8,  124,   14,  286,  170,    8,  157,   46,    5,   27,\n",
              "        239,   16,  179,    2,   38,   32,   25, 7944,  451,  202,   14,\n",
              "          6,  717,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "fiELLqYLQjX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "737928c5-fffc-43ab-e833-591549bab4e8"
      },
      "cell_type": "code",
      "source": [
        "original_word_index[\"evolved\"],word_index[\"evolved\"] # difference of 3 because of the special tokens"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9286, 9289)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "WThJQKiFQjYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b1599ac-72c1-4452-82ea-7711dc454d0c"
      },
      "cell_type": "code",
      "source": [
        "len(original_word_index)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "-pZZKJXcQjYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5b8f57a-9a92-44d7-a10e-655f0f8ae578"
      },
      "cell_type": "code",
      "source": [
        "words[:10]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[':START:', 'please', 'give', 'this', 'one', 'a', 'miss', 'br', 'br', ':UNK:']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "_UVMeAh_QjYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save Visualization as HTML"
      ]
    },
    {
      "metadata": {
        "id": "lWDTLd8BQjYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save visualization as HTML\n",
        "with open(\"visualization.html\", \"w\") as html_file:\n",
        "    for word, alpha in zip(words, alphas_values / alphas_values.max()):\n",
        "        if word == \":START:\":\n",
        "            continue\n",
        "        elif word == \":PAD:\":\n",
        "            break\n",
        "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\">%s</font>\\n' % (alpha, word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8thW710qQjYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "653071e8-6cb5-4952-dc50-a42fc7c1402e"
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "import codecs\n",
        "display(HTML(codecs.open(\"visualization.html\",'r').read()))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<font style=\"background: rgba(255, 255, 0, 0.346904)\">please</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.365231)\">give</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.475588)\">this</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.540483)\">one</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.398431)\">a</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.185081)\">miss</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.171780)\">br</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.154044)\">br</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.113310)\">:UNK:</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.107675)\">:UNK:</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.236995)\">and</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.062461)\">the</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.030342)\">rest</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.045664)\">of</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.009896)\">the</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.012283)\">cast</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.005885)\">rendered</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.018689)\">terrible</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.010101)\">performances</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.006152)\">the</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.008148)\">show</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.037995)\">is</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.021599)\">flat</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.022713)\">flat</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.024875)\">flat</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.045603)\">br</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.043000)\">br</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.026250)\">i</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.027060)\">don't</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.029049)\">know</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.010496)\">how</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.021402)\">michael</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.018124)\">madison</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.017246)\">could</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.010851)\">have</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.020079)\">allowed</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.074389)\">this</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.088008)\">one</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.090406)\">on</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.151191)\">his</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.057129)\">plate</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.011412)\">he</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.027686)\">almost</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.032151)\">seemed</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.035784)\">to</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.049806)\">know</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.200155)\">this</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.172197)\">wasn't</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.149860)\">going</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.154385)\">to</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.130556)\">work</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.364070)\">out</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 1.000000)\">and</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.387987)\">his</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.575106)\">performance</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.578013)\">was</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.364779)\">quite</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.475008)\">:UNK:</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.188142)\">so</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.283182)\">all</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.148992)\">you</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.141191)\">madison</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.124843)\">fans</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.251853)\">give</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.275784)\">this</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.160072)\">a</font>\n",
              "<font style=\"background: rgba(255, 255, 0, 0.059655)\">miss</font>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gbs_puu8QjYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}